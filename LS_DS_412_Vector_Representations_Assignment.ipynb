{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_412_Vector_Representations_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pingao2019/DS-Unit-4-Sprint-1-NLP/blob/master/LS_DS_412_Vector_Representations_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtHN0Vf6ID5_",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Vector Representations\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hyj-f9FDcVFp",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7bcmqfGXrFG"
      },
      "source": [
        "## 1) *Clean:* Job Listings from indeed.com that contain the title \"Data Scientist\" \n",
        "\n",
        "You have `job_listings.csv` in the data folder for this module. The text data in the description column is still messy - full of html tags. Use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to clean up this column. You will need to read thru the documentation to accomplish this task. \n",
        "\n",
        "`Tip:` You will need to install the `bs4` library inside your conda environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTM1-0bID6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./data/job_listings.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcYlc1URXhlC",
        "colab": {},
        "outputId": "f8b86477-af05-4f1c-9af1-59ac3bb65adf"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (<ipython-input-1-25a06447bdf3>, line 5)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-25a06447bdf3>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\"\u001b[0m\n\u001b[1;37m                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5C4xFZNtX1m2"
      },
      "source": [
        "## 2) Use Spacy to tokenize the listings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhUHuMr-X-II",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lgCZNL_YycP"
      },
      "source": [
        "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2PZ8Pj_YxcF",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zo1iH_UeY7_n"
      },
      "source": [
        "## 4) Visualize the most common word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M5LB00uyZKV5",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bwFsTqrVZMYi"
      },
      "source": [
        "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gx2gZCbl5Np",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39dlJsCID7z",
        "colab_type": "text"
      },
      "source": [
        "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "m4VKakVlID71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiDfTWceoRkH"
      },
      "source": [
        "## Stretch Goals\n",
        "\n",
        " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
        " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
        " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
        " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
        "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
        " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p797RMDID8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vector Representations\n",
        "Data Science Unit 4 Sprint 2 Assignment 2\n",
        "In [2]:\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "1) Clean: Job Listings from indeed.com that contain the title \"Data Scientist\"\n",
        "\n",
        "You have job_listings.csv in the data folder for this module. The text data in the description column is still messy - full of html tags. Use the BeautifulSoup library to clean up this column. You will need to read through the documentation to accomplish this task.\n",
        "In [8]:\n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "##### Your Code Here #####\n",
        "#raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\")\n",
        "df = pd.read_csv('./data/job_listings.csv')\n",
        "\n",
        "In [4]:\n",
        "\n",
        "def clean_description(desc):\n",
        "    soup = BeautifulSoup(desc)\n",
        "    return soup.get_text()\n",
        "df['clean_desc'] = df['description'].apply(clean_description)\n",
        "df = df.drop(columns='Unnamed: 0')\n",
        "df.head()\n",
        "\n",
        "Out[4]:\n",
        "\tdescription \ttitle \tclean_desc\n",
        "0 \tb\"<div><div>Job Requirements:</div><ul><li><p>... \tData scientist \tb\"Job Requirements:\\nConceptual understanding ...\n",
        "1 \tb'<div>Job Description<br/>\\n<br/>\\n<p>As a Da... \tData Scientist I \tb'Job Description\\n\\nAs a Data Scientist 1, yo...\n",
        "2 \tb'<div><p>As a Data Scientist you will be work... \tData Scientist - Entry Level \tb'As a Data Scientist you will be working on c...\n",
        "3 \tb'<div class=\"jobsearch-JobMetadataHeader icl-... \tData Scientist \tb'$4,969 - $6,756 a monthContractUnder the gen...\n",
        "4 \tb'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ... \tData Scientist \tb'Location: USA \\xe2\\x80\\x93 multiple location...\n",
        "In [12]:\n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "def remove_unicode_chars(df):\n",
        "    df = df.copy()\n",
        "    df['description'] = df['description'].str.replace(r'(\\\\(x|n)[a-z0-9]{0,2})', '')\n",
        "    return df\n",
        "\n",
        "def remove_html(df):\n",
        "    df = df.copy()\n",
        "    df['description'] = df['description'].apply(lambda x: bs(x).get_text().replace(\"\\\\n\", \" \"))\n",
        "    df['description'] = df['description'].str.replace('b\"', '')\n",
        "    df['description'] = df['description'].str.replace(\"b'\", '')\n",
        "    df['description'] = df['description'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
        "    df['description'] = df['description'].str.lower()\n",
        "    df['tokenized'] = df['description'].apply(lambda x: [token.text for token in nlp(x) \n",
        "                                                         if (token.is_stop is False) & (token.is_punct is False)])\n",
        "    return df\n",
        "\n",
        "In [13]:\n",
        "\n",
        "df1 = remove_unicode_chars(df)\n",
        "df1 = remove_html(df1)\n",
        "df1\n",
        "\n",
        "Out[13]:\n",
        "\tUnnamed: 0 \tdescription \ttitle \ttokenized\n",
        "0 \t0 \tjob requirements:conceptual understanding in m... \tData scientist \t[job, requirements, conceptual, understanding,...\n",
        "1 \t1 \tjob descriptionas a data scientist , you will ... \tData Scientist I \t[job, descriptionas, data, scientist, help, bu...\n",
        "2 \t2 \tas a data scientist you will be working on con... \tData Scientist - Entry Level \t[data, scientist, working, consulting, busines...\n",
        "3 \t3 \t$, - $, a monthcontractunder the general super... \tData Scientist \t[$, $, monthcontractunder, general, supervisio...\n",
        "4 \t4 \tlocation: usa multiple locations+ years of an... \tData Scientist \t[location, usa, , multiple, locations+, years...\n",
        "... \t... \t... \t... \t...\n",
        "421 \t421 \tabout us:want to be part of a fantastic and fu... \tSenior Data Science Engineer \t[want, fantastic, fun, startup, s, revolutioni...\n",
        "422 \t422 \tinternshipat uber, we ignite opportunity by se... \t2019 PhD Data Scientist Internship - Forecasti... \t[internshipat, uber, ignite, opportunity, sett...\n",
        "423 \t423 \t$, - $, a yeara million people a year die in c... \tData Scientist - Insurance \t[$, $, yeara, million, people, year, die, car,...\n",
        "424 \t424 \tsenior data scientistjob descriptionabout usam... \tSenior Data Scientist \t[senior, data, scientistjob, descriptionabout,...\n",
        "425 \t425 \tcerner intelligence is a new, innovative organ... \tData Scientist \t[cerner, intelligence, new, innovative, organi...\n",
        "\n",
        "426 rows × 4 columns\n",
        "2) Use Spacy to tokenize the listings\n",
        "In [17]:\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "doc = nlp(str(df['clean_desc']))\n",
        "\n",
        "Out[17]:\n",
        "\n",
        "spacy.tokens.doc.Doc\n",
        "\n",
        "In [20]:\n",
        "\n",
        "print([token.lemma_ for token in doc[:10] if (token.is_stop != True) and (token.is_punct != True)])\n",
        "\n",
        "['0', '     ', 'b\"job', 'Requirements:\\\\nConceptual', 'understanding', '\\n', '1', '     ', \"b'Job\"]\n",
        "\n",
        "In [22]:\n",
        "\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "\n",
        "In [23]:\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['clean_desc'], batch_size=500):\n",
        "    \n",
        "    doc_tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if (token.text.lower() not in STOP_WORDS) & (token.is_digit is False) & (token.is_punct is False):\n",
        "            doc_tokens.append(token.text.lower())\n",
        "            \n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['tokens'] = tokens\n",
        "\n",
        "In [24]:\n",
        "\n",
        "df.head()\n",
        "\n",
        "Out[24]:\n",
        "\tdescription \ttitle \tclean_desc \ttokens\n",
        "0 \tb\"<div><div>Job Requirements:</div><ul><li><p>... \tData scientist \tb\"Job Requirements:\\nConceptual understanding ... \t[b\"job, requirements:\\nconceptual, understandi...\n",
        "1 \tb'<div>Job Description<br/>\\n<br/>\\n<p>As a Da... \tData Scientist I \tb'Job Description\\n\\nAs a Data Scientist 1, yo... \t[b'job, description\\n\\nas, data, scientist, 1,...\n",
        "2 \tb'<div><p>As a Data Scientist you will be work... \tData Scientist - Entry Level \tb'As a Data Scientist you will be working on c... \t[b'as, data, scientist, working, consulting, b...\n",
        "3 \tb'<div class=\"jobsearch-JobMetadataHeader icl-... \tData Scientist \tb'$4,969 - $6,756 a monthContractUnder the gen... \t[b'$4,969, $6,756, monthcontractunder, general...\n",
        "4 \tb'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ... \tData Scientist \tb'Location: USA \\xe2\\x80\\x93 multiple location... \t[b'location:, usa, \\xe2\\x80\\x93, multiple, loc...\n",
        "3) Use Scikit-Learn's CountVectorizer to get word counts for each listing.\n",
        "In [39]:\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer()\n",
        "\n",
        "vect.fit(df['clean_desc'])\n",
        "\n",
        "# document term matrix\n",
        "dtm = vect.transform(df['clean_desc'])\n",
        "\n",
        "In [29]:\n",
        "\n",
        "# This is a sparse matrix with only present values (no 0s)\n",
        "print(dtm)\n",
        "\n",
        "  (0, 225)\t1\n",
        "  (0, 498)\t1\n",
        "  (0, 569)\t2\n",
        "  (0, 608)\t2\n",
        "  (0, 661)\t1\n",
        "  (0, 675)\t1\n",
        "  (0, 697)\t1\n",
        "  (0, 755)\t1\n",
        "  (0, 914)\t1\n",
        "  (0, 1071)\t1\n",
        "  (0, 1472)\t1\n",
        "  (0, 1528)\t1\n",
        "  (0, 1612)\t1\n",
        "  (0, 1689)\t1\n",
        "  (0, 2071)\t1\n",
        "  (0, 2132)\t1\n",
        "  (0, 2167)\t1\n",
        "  (0, 2482)\t1\n",
        "  (0, 2616)\t1\n",
        "  (0, 2848)\t1\n",
        "  (0, 2960)\t2\n",
        "  (0, 2977)\t1\n",
        "  (0, 3133)\t1\n",
        "  (0, 3160)\t1\n",
        "  (0, 3249)\t1\n",
        "  :\t:\n",
        "  (425, 9654)\t1\n",
        "  (425, 9690)\t1\n",
        "  (425, 9711)\t1\n",
        "  (425, 9759)\t1\n",
        "  (425, 9783)\t2\n",
        "  (425, 9820)\t1\n",
        "  (425, 9826)\t1\n",
        "  (425, 9828)\t1\n",
        "  (425, 9834)\t2\n",
        "  (425, 9847)\t3\n",
        "  (425, 9849)\t1\n",
        "  (425, 9863)\t10\n",
        "  (425, 9865)\t2\n",
        "  (425, 9866)\t1\n",
        "  (425, 9876)\t6\n",
        "  (425, 9882)\t1\n",
        "  (425, 9885)\t1\n",
        "  (425, 9888)\t1\n",
        "  (425, 9911)\t5\n",
        "  (425, 9924)\t2\n",
        "  (425, 9960)\t3\n",
        "  (425, 10028)\t5\n",
        "  (425, 10041)\t2\n",
        "  (425, 10052)\t3\n",
        "  (425, 10054)\t1\n",
        "\n",
        "In [40]:\n",
        "\n",
        "# Make it pretty with a DataFrame, first need to make it dense\n",
        "# the columns come from an attribute on the CountVectorizer object\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm1 = dtm.copy()\n",
        "\n",
        "dtm1\n",
        "\n",
        "Out[40]:\n",
        "\t00 \t000 \t02115 \t03 \t0356 \t04 \t062 \t06366 \t08 \t10 \t... \tzenreach \tzero \tzeus \tzf \tzheng \tzillow \tzones \tzoom \tzuckerberg \tzurich\n",
        "0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "2 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "3 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t1 \t0 \t0 \t0 \t0 \t0\n",
        "4 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t...\n",
        "421 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "422 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "423 \t0 \t2 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "424 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "425 \t0 \t1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "\n",
        "426 rows × 10069 columns\n",
        "In [ ]:\n",
        "\n",
        "# Try and get rid of some of those numbers\n",
        "cols_2drop = []\n",
        "for col in dtm.columns:\n",
        "    if col.isnumeric():\n",
        "        cols_2drop.append(col)\n",
        "cols_2drop\n",
        "\n",
        "In [56]:\n",
        "\n",
        "dtm2 = dtm1.drop(columns=cols_2drop)\n",
        "dtm2\n",
        "# Still some numbers, probably have to use regex\n",
        "\n",
        "Out[56]:\n",
        "\t100k \t100x \t10b \t10ms \t10x \t110k \t1324b \t159m \t169334br \t17b \t... \tzenreach \tzero \tzeus \tzf \tzheng \tzillow \tzones \tzoom \tzuckerberg \tzurich\n",
        "0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "2 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "3 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t1 \t0 \t0 \t0 \t0 \t0\n",
        "4 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t... \t...\n",
        "421 \t0 \t0 \t1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "422 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "423 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t1 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "424 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "425 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t... \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0 \t0\n",
        "\n",
        "426 rows × 9890 columns\n",
        "4) Visualize the most common word counts\n",
        "In [58]:\n",
        "\n",
        "from collections import Counter\n",
        "def count(docs):\n",
        "    \"\"\"\n",
        "    Function that takes a corpus of document and returns and dataframe of word counts for us to analyze.\n",
        "    \"\"\"\n",
        "    word_counts = Counter()\n",
        "    appears_in = Counter()\n",
        "        \n",
        "    total_docs = len(docs)\n",
        "\n",
        "    for doc in docs:\n",
        "        word_counts.update(doc)\n",
        "        appears_in.update(set(doc))\n",
        "\n",
        "    temp = zip(word_counts.keys(), word_counts.values())\n",
        "        \n",
        "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
        "        \n",
        "    wc = wc.sort_values(by='rank')\n",
        "    wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "    t2 = zip(appears_in.keys(), appears_in.values())\n",
        "    ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "    wc = ac.merge(wc, on='word')\n",
        "\n",
        "    wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "        \n",
        "    return wc.sort_values(by='rank')\n",
        "\n",
        "In [59]:\n",
        "\n",
        "word_count = count(df.tokens)\n",
        "word_count.head()\n",
        "\n",
        "Out[59]:\n",
        "\tword \tappears_in \tcount \trank \tpct_total \tcul_pct_total \tappears_in_pct\n",
        "40 \tdata \t419 \t3751 \t1.0 \t0.030037 \t0.030037 \t0.983568\n",
        "201 \tbusiness \t303 \t1008 \t2.0 \t0.008072 \t0.038108 \t0.711268\n",
        "22 \texperience \t357 \t941 \t3.0 \t0.007535 \t0.045643 \t0.838028\n",
        "49 \twork \t326 \t876 \t4.0 \t0.007015 \t0.052658 \t0.765258\n",
        "82 \tteam \t314 \t726 \t5.0 \t0.005814 \t0.058472 \t0.737089\n",
        "In [64]:\n",
        "\n",
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc_top20 = word_count[word_count['rank'] <=20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['pct_total'], label=wc_top20['word'], alpha=.7)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix\n",
        "In [66]:\n",
        "\n",
        "# Term Frequency-Inverse Document Frequency will give scores to word counts based on uniqueness\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=500)\n",
        "\n",
        "# Create a vocabulary\n",
        "# Reminder: The vocabulary establishes all of the possible words that we might use.\n",
        "# The vocabulary dictionary does not represent the counts of words. We use the original docs here\n",
        "dtm_tf = tfidf.fit_transform(df['clean_desc'])\n",
        "\n",
        "dtm_tf = pd.DataFrame(dtm_tf.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "dtm_tf.head()\n",
        "\n",
        "Out[66]:\n",
        "\tability \table \taccess \tachieve \taction \tactionable \tad \taddress \tadvanced \tage \t... \tx99 \tx99ll \tx99re \tx99s \tx9d \txc2 \txe2 \txef \tyear \tyears\n",
        "0 \t0.120724 \t0.0 \t0.0 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.0 \t0.000000 \t0.000000 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.266181 \t0.000000 \t0.0 \t0.000000 \t0.00000\n",
        "1 \t0.038988 \t0.0 \t0.0 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.0 \t0.000000 \t0.000000 \t... \t0.115409 \t0.0 \t0.124717 \t0.065169 \t0.0 \t0.000000 \t0.220322 \t0.0 \t0.064169 \t0.00000\n",
        "2 \t0.000000 \t0.0 \t0.0 \t0.0 \t0.000000 \t0.154847 \t0.0 \t0.0 \t0.117431 \t0.000000 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.00000\n",
        "3 \t0.000000 \t0.0 \t0.0 \t0.0 \t0.121405 \t0.000000 \t0.0 \t0.0 \t0.000000 \t0.099024 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.124930 \t0.00000\n",
        "4 \t0.000000 \t0.0 \t0.0 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.0 \t0.000000 \t0.000000 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.159148 \t0.0 \t0.000000 \t0.17264\n",
        "\n",
        "5 rows × 500 columns\n",
        "In [67]:\n",
        "\n",
        "# We can use spacy to do the tokenization, and use bigram words\n",
        "def tokenize(document):\n",
        "    \"\"\" get the lemma and tokenize\"\"\"\n",
        "    \n",
        "    doc = nlp(document)\n",
        "    \n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop is False) and (token.is_punct is False)]\n",
        "\n",
        "In [69]:\n",
        "\n",
        "# instanstiate tfidf with bigram and spacy tokenizer\n",
        "tfidf_spacy = TfidfVectorizer(stop_words='english',\n",
        "                        ngram_range=(1,2),\n",
        "                        max_df=0.97,\n",
        "                        min_df=0.3,\n",
        "                        tokenizer=tokenize)\n",
        "dtm_tf_spacy = tfidf_spacy.fit_transform(df['clean_desc'])\n",
        "\n",
        "# make a df out of the sparse matrix and get features from the original object\n",
        "dtm_tf_spacy = pd.DataFrame(dtm_tf_spacy.todense(), columns=tfidf_spacy.get_feature_names())\n",
        "\n",
        "dtm_tf_spacy.head()\n",
        "\n",
        "Out[69]:\n",
        "\t+ \t+ year \tability \tadvanced \talgorithm \tanalysis \tanalytic \tanalytical \tanalyze \tapply \t... \ttool \tunderstand \tunderstanding \tuse \tvalue \tveteran \twork \tworld \twrite \tyear\n",
        "0 \t0.000000 \t0.000000 \t0.186097 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.000000 \t0.000000 \t... \t0.000000 \t0.0 \t0.215759 \t0.000000 \t0.0 \t0.000000 \t0.119717 \t0.000000 \t0.206853 \t0.000000\n",
        "1 \t0.084258 \t0.092003 \t0.077247 \t0.000000 \t0.085863 \t0.0 \t0.000000 \t0.079303 \t0.000000 \t0.000000 \t... \t0.066975 \t0.0 \t0.089559 \t0.000000 \t0.0 \t0.000000 \t0.198774 \t0.160022 \t0.171725 \t0.060464\n",
        "2 \t0.000000 \t0.000000 \t0.000000 \t0.175093 \t0.000000 \t0.0 \t0.000000 \t0.160178 \t0.176223 \t0.145832 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.200743 \t0.000000 \t0.000000 \t0.000000\n",
        "3 \t0.000000 \t0.000000 \t0.000000 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.180411 \t0.000000 \t... \t0.000000 \t0.0 \t0.000000 \t0.178113 \t0.0 \t0.187053 \t0.205514 \t0.000000 \t0.000000 \t0.125028\n",
        "4 \t0.387274 \t0.422874 \t0.000000 \t0.000000 \t0.000000 \t0.0 \t0.311096 \t0.000000 \t0.000000 \t0.000000 \t... \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.0 \t0.000000 \t0.000000 \t0.000000 \t0.000000 \t0.277910\n",
        "\n",
        "5 rows × 120 columns\n",
        "6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings.\n",
        "In [70]:\n",
        "\n",
        "# Nearest neighbors identifies a centroid to compare close points\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=6, algorithm='kd_tree')\n",
        "nn.fit(dtm_tf_spacy)\n",
        "\n",
        "Out[70]:\n",
        "\n",
        "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
        "                 metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
        "                 radius=1.0)\n",
        "\n",
        "In [75]:\n",
        "\n",
        "# This shows the distances and similar docs compared to the first doc\n",
        "nn.kneighbors([dtm_tf_spacy.iloc[0]])\n",
        "\n",
        "Out[75]:\n",
        "\n",
        "(array([[0.        , 0.97082508, 1.01227969, 1.019085  , 1.02245685,\n",
        "         1.02358835]]),\n",
        " array([[  0, 345,  56, 419,  49, 336]], dtype=int64))\n",
        "\n",
        "In [87]:\n",
        "\n",
        "df['clean_desc'][345][:1000]\n",
        "\n",
        "Out[87]:\n",
        "\n",
        "\"b'Small businesses are the backbone of the American economy, providing innovation, growth, and job opportunities to the communities around them. Many small businesses need funds to run and grow their business, but often don\\\\xe2\\\\x80\\\\x99t get the financial help they need from traditional banks. That\\\\xe2\\\\x80\\\\x99s where BlueVine comes in. BlueVine is a leading provider of small business financing. We\\\\xe2\\\\x80\\\\x99re using cutting edge technology to disrupt the traditional banking industry and develop the next generation of fast and simple financial products, designed for today\\\\xe2\\\\x80\\\\x99s small businesses. BlueVine is headquartered in Redwood City, CA and backed by leading investors including Menlo Ventures, Citi Ventures, SVB Financial, Nationwide Insurance, M12 (Microsoft\\\\xe2\\\\x80\\\\x99s Venture Arm).\\\\n\\\\nBlueVine is searching for a Junior Data Scientist, who will be responsible for the critical decision-making processes driving our automation: risk analysis, credibility scoring, fraud detect\"\n",
        "\n",
        "In [78]:\n",
        "\n",
        "df['clean_desc'][0]\n",
        "# yeah they have stats math cs in common\n",
        "\n",
        "Out[78]:\n",
        "\n",
        "'b\"Job Requirements:\\\\nConceptual understanding in Machine Learning models like Nai\\\\xc2\\\\xa8ve Bayes, K-Means, SVM, Apriori, Linear/ Logistic Regression, Neural, Random Forests, Decision Trees, K-NN along with hands-on experience in at least 2 of them\\\\nIntermediate to expert level coding skills in Python/R. (Ability to write functions, clean and efficient data manipulation are mandatory for this role)\\\\nExposure to packages like NumPy, SciPy, Pandas, Matplotlib etc in Python or GGPlot2, dplyr, tidyR in R\\\\nAbility to communicate Model findings to both Technical and Non-Technical stake holders\\\\nHands on experience in SQL/Hive or similar programming language\\\\nMust show past work via GitHub, Kaggle or any other published article\\\\nMaster\\'s degree in Statistics/Mathematics/Computer Science or any other quant specific field.\\\\nApply Now\"'\n",
        "\n",
        "In [91]:\n",
        "\n",
        "# Query a specified sample listing\n",
        "\n",
        "sample = [\"\"\"Generous pay for capable data scientist. Responsibilities include data analysis, data engineering, using tree based models. \n",
        "Will need to generate reports to techinical managers and visualizations as well as develop apps with a team.\n",
        "I hope the shape is somewhat different.\n",
        "\"\"\"]\n",
        "# Introduce the new sample to transform the tfidf object \n",
        "# new_query = tfidf.transform(sample)\n",
        "# nn.kneighbors(new_query.todense())\n",
        "\n",
        "In [ ]:\n",
        "\n",
        "\n",
        "Stretch Goals\n",
        "\n",
        "    Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
        "    Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
        "    Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
        "    Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
        "        Hint: K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
        "    Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :)\n",
        "\n",
        "In [ ]:\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}